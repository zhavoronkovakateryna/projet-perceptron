# -*- coding: utf-8 -*-
#################################### Import ####################################
import numpy as np
import matplotlib.pyplot as plt

#################################### Main ######################################
class MLP:
    def __init__(self, position, fonction=lambda x: x, theta=[], gradient=0, data=[])->None:
        """
        Fonction d'initialisation d'un neurones du réseau MLP
        Parameters
        ----------
        position : list
            de la forme [(couche, n° dans la couche),..].
        fonction : fonction, fonction d'activation du neurone
            The default is lambda x: x.
        theta : list, les poids et le biais associé à ce neurone
            The default is [].
        gradient : float, formule du calcul du gradient du neurone par rapport à ceux de la couche précedentes
            The default is 0.
        data : list, optional
            The default is [].
        Returns
        -------
        None.
        """
        self.position = position
        self.gradient = gradient
        self.theta = np.array(theta) if len(theta)>0 else np.array([])
        self.data = np.array(data)
        self.fonction = fonction
        self.activation = 0  # Stocke z apres l'activation du neurone
        self.s = 0  # Stocke z (avant activation)
        
    def __repr__(self):
        return f"|position : {self.position[0]},{self.position[1]} __ paramètre : {self.theta}|"
    
    def data_entry(self)->None:
        """
        Fonction demandant et affecte le array au perceptron <self>
        Returns
        -------
        None.
        """
        data = eval(input('Transmettez vos données: '))
        self.data = np.array(data)
    
    def calculer_sortie(self, entrees):
        """
        Calcule la sortie du neurone
        s = sum(theta_i * entree_i) + biais
        activation = fonction(s)
        
        Parameters
        ----------
        entrees : np.array
            Les activations de la couche précédente
            
        Returns
        -------
        float : l'activation du neurone
        """
        # theta[:-1] contient les poids, theta[-1] contient le biais
        self.s = np.dot(self.theta[:-1], entrees) + self.theta[-1]
        self.activation = self.fonction(self.s)
        return self.activation


def créer_mlp(nb_neurone_couche, fonction_couche, theta_neurone_couche)->dict:
    """
    Créer un réseau MLP de <len(nb_neurone_couche)> couches, avec <nb_neurone_couche[i]> neurones 
    à la couche i muni de la fonction d'activation <fonction_couche[i]>
    
    Parameters
    ----------
    nb_neurone_couche : list
        de la forme [nb_neurone_couche_1,nb_neurone_couche_2,..].
    fonction_couche : list
        de la forme [fonction_couche_1,fonction_couche_2,..].
    theta_neurone_couche : list
        de la forme [[theta_11,theta,12,..], [theta_21,theta,22,..], ..].
    Returns
    -------
    dict
            Réseau MLP.
    """
    reseaux = {}
    for i in range(len(nb_neurone_couche)):
        for j in range(nb_neurone_couche[i]):
            nom = "N_" + f"{i+1}{j+1}"
            reseaux[nom] = MLP((i+1, j+1), fonction_couche[i], theta_neurone_couche[i][j])
    return reseaux


def initialiser_poids(nb_neurone_couche):
    """
    Initialise aléatoirement les poids pour tout le réseau
    
    Parameters
    ----------
    nb_neurone_couche : list
        [nb_entrees, nb_cache1, ..., nb_sortie]
        
    Returns
    -------
    list : theta pour toutes les couches
    """
    theta_couches = []
    for i in range(len(nb_neurone_couche)):
        theta_couche = []
        for j in range(nb_neurone_couche[i]):
            if i == 0:
                # Couche d'entrée : pas de poids
                theta_couche.append([])
            else:
                # nb_entrees + 1 pour le biais
                nb_entrees = nb_neurone_couche[i-1]
                theta = np.random.randn(nb_entrees + 1) * 0.5
                theta_couche.append(theta)
        theta_couches.append(theta_couche)
    return theta_couches


def propagation_avant(reseaux, entree, nb_neurone_couche):
    """
    Effectue la propagation avant dans le réseau
    
    Parameters
    ----------
    reseaux : dict
        Le réseau MLP
    entree : list/np.array
        Les valeurs d'entrée
    nb_neurone_couche : list
        Architecture du réseau
        
    Returns
    -------
    None (met à jour les activations dans le réseau)
    """
    # Initialiser la couche d'entrée
    for j in range(nb_neurone_couche[0]):
        nom = f"N_1{j+1}"
        reseaux[nom].activation = entree[j]
    
    # Propager à travers les couches
    for i in range(1, len(nb_neurone_couche)):
        # Récupérer les activations de la couche précédente
        activations_prec = []
        for j in range(nb_neurone_couche[i-1]):
            nom = f"N_{i}{j+1}"
            activations_prec.append(reseaux[nom].activation)
        activations_prec = np.array(activations_prec)
        
        # Calculer les activations de la couche actuelle
        for j in range(nb_neurone_couche[i]):
            nom = f"N_{i+1}{j+1}"
            reseaux[nom].calculer_sortie(activations_prec)


def calculer_gradient_sortie(reseaux, y_attendu, nb_neurone_couche, derivee_fonction):
    """
    Calcule les gradients pour la couche de sortie
    delta = (y_pred - y_attendu) * g'(s)
    
    Parameters
    ----------
    reseaux : dict
        Le réseau MLP
    y_attendu : list/np.array
        Sortie attendue
    nb_neurone_couche : list
        Architecture
    derivee_fonction : function
        Dérivée de la fonction d'activation
    """
    couche_sortie = len(nb_neurone_couche)
    for j in range(nb_neurone_couche[-1]):
        nom = f"N_{couche_sortie}{j+1}"
        neurone = reseaux[nom]
        neurone.gradient = (neurone.activation - y_attendu[j]) * derivee_fonction(neurone.s)


def retropropagation_gradient(reseaux, nb_neurone_couche, derivee_fonction):
    """
    Rétropropage les gradients vers les couches cachées
    delta_i = sum(delta_j * w_ji) * g'(s_i)
    
    Parameters
    ----------
    reseaux : dict
        Le réseau MLP
    nb_neurone_couche : list
        Architecture
    derivee_fonction : function
        Dérivée de la fonction d'activation
    """
    # Parcourir les couches de l'avant-dernière vers la première cachée
    for i in range(len(nb_neurone_couche) - 2, 0, -1):
        for j in range(nb_neurone_couche[i]):
            nom = f"N_{i+1}{j+1}"
            neurone = reseaux[nom]
            
            # Somme des gradients pondérés de la couche suivante
            somme = 0
            for k in range(nb_neurone_couche[i+1]):
                nom_suivant = f"N_{i+2}{k+1}"
                neurone_suivant = reseaux[nom_suivant]
                # theta[j] correspond au poids de connexion j
                somme += neurone_suivant.gradient * neurone_suivant.theta[j]
            
            neurone.gradient = somme * derivee_fonction(neurone.s)


def mise_a_jour_poids(reseaux, nb_neurone_couche, eta):
    """
    Met à jour les poids selon la règle de gradient
    theta = theta - eta * gradient * activation_precedente
    
    Parameters
    ----------
    reseaux : dict
        Le réseau MLP
    nb_neurone_couche : list
        Architecture
    eta : float
        Taux d'apprentissage
    """
    for i in range(1, len(nb_neurone_couche)):
        # Récupérer activations de la couche précédente
        activations_prec = []
        for j in range(nb_neurone_couche[i-1]):
            nom = f"N_{i}{j+1}"
            activations_prec.append(reseaux[nom].activation)
        activations_prec = np.array(activations_prec)
        
        # Mettre à jour les poids de la couche actuelle
        for j in range(nb_neurone_couche[i]):
            nom = f"N_{i+1}{j+1}"
            neurone = reseaux[nom]
            
            # Mise à jour des poids
            for k in range(len(activations_prec)):
                neurone.theta[k] -= eta * neurone.gradient * activations_prec[k]
            
            # Mise à jour du biais
            neurone.theta[-1] -= eta * neurone.gradient


def entrainer(reseaux, donnees, nb_neurone_couche, derivee_fonction, eta, nb_epoques):
    """
    Entraîne le réseau sur les données
    
    Parameters
    ----------
    reseaux : dict
        Le réseau MLP
    donnees : list
        Liste de tuples (entree, sortie_attendue)
    nb_neurone_couche : list
        Architecture
    derivee_fonction : function
        Dérivée de la fonction d'activation
    eta : float
        Taux d'apprentissage
    nb_epoques : int
        Nombre d'époques
    """
    erreurs = []
    
    for epoque in range(nb_epoques):
        erreur_totale = 0
        
        for entree, sortie_attendue in donnees:
            # Propagation avant
            propagation_avant(reseaux, entree, nb_neurone_couche)
            
            # Calcul de l'erreur
            for j in range(nb_neurone_couche[-1]):
                nom = f"N_{len(nb_neurone_couche)}{j+1}"
                pred = reseaux[nom].activation
                erreur_totale += (pred - sortie_attendue[j])**2
            
            # Rétropropagation
            calculer_gradient_sortie(reseaux, sortie_attendue, nb_neurone_couche, derivee_fonction)
            retropropagation_gradient(reseaux, nb_neurone_couche, derivee_fonction)
            mise_a_jour_poids(reseaux, nb_neurone_couche, eta)
        
        erreurs.append(erreur_totale / len(donnees))
        
        if epoque % 100 == 0:
            print(f"Epoque {epoque}: Erreur = {erreurs[-1]:.6f}")
    
    return erreurs


def prediction(reseaux, entree, nb_neurone_couche):
    """
    Fait une prédiction avec le réseau entraîné
    
    Parameters
    ----------
    reseaux : dict
        Le réseau MLP
    entree : list/np.array
        Entrée
    nb_neurone_couche : list
        Architecture
        
    Returns
    -------
    list : Les activations de la couche de sortie
    """
    propagation_avant(reseaux, entree, nb_neurone_couche)
    
    sortie = []
    for j in range(nb_neurone_couche[-1]):
        nom = f"N_{len(nb_neurone_couche)}{j+1}"
        sortie.append(reseaux[nom].activation)
    
    return sortie


#################################### Fonction de test ####################################

# Fonction sigmoid et sa dérivée
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_prime(x):
    s = sigmoid(x)
    return s * (1 - s)

# Exemple XOR
print("=" * 60)
print("TEST DU RESEAU MLP SUR LE PROBLEME XOR")
print("=" * 60)

# Architecture: 2 entrées, 2 neurones cachés, 1 sortie
nb_neurone_couche = [2, 2, 1]
fonction_couche = [lambda x: x, sigmoid, sigmoid]  # Identité pour l'entrée

# Initialiser les poids
theta_neurone_couche = initialiser_poids(nb_neurone_couche)

# Créer le réseau
reseaux = créer_mlp(nb_neurone_couche, fonction_couche, theta_neurone_couche)

print(f"\nArchitecture: {nb_neurone_couche}")
print("Réseau créé avec succès!")
print(f"Nombre de neurones: {len(reseaux)}")

# Données XOR
donnees = [
    ([0, 0], [0]),
    ([0, 1], [1]),
    ([1, 0], [1]),
    ([1, 1], [0])
]

# Entraînement
print("\nEntraînement en cours...")
eta = 0.5
nb_epoques = 5000
erreurs = entrainer(reseaux, donnees, nb_neurone_couche, sigmoid_prime, eta, nb_epoques)

# Résultats
print("\n" + "=" * 60)
print("RESULTATS")
print("=" * 60)
for entree, sortie_attendue in donnees:
    pred = prediction(reseaux, entree, nb_neurone_couche)
    print(f"Entree: {entree} -> Attendu: {sortie_attendue[0]:.0f} | Predit: {pred[0]:.4f}")

# Visualisation
plt.figure(figsize=(10, 5))
plt.plot(erreurs)
plt.xlabel('Epoque')
plt.ylabel('Erreur MSE')
plt.title('Evolution de l\'erreur pendant l\'entrainement')
plt.grid(True)
plt.show()

print("\n" + "=" * 60)

